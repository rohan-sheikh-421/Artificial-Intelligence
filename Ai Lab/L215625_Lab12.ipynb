{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import numpy as np"
      ],
      "metadata": {
        "id": "MbufmoLJ-KjL"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "x = np.array([-5, -4, -3, -2, -1, 0, 1, 2, 3, 4, 5])\n",
        "y = np.array([5.5, 2, -0.5, -2, -2.5, -2, -0.5, 2, 5.5, 10, 15.5])\n"
      ],
      "metadata": {
        "id": "E64XIMSF9iDa"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Initialize weights and biases\n",
        "input_size = 1\n",
        "hidden_size = 10\n",
        "output_size = 1\n",
        "\n",
        "# Initialize weights based on the range of the input data\n",
        "w_hidden = np.random.uniform(-1 * np.max(np.abs(x)), 1 * np.max(np.abs(x)), size=(input_size, hidden_size))\n",
        "b_hidden = np.random.uniform(-1 * np.max(np.abs(x)), 1 * np.max(np.abs(x)), size=(1, hidden_size))\n",
        "w_output = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(hidden_size, output_size))\n",
        "b_output = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(1, output_size))"
      ],
      "metadata": {
        "id": "gVBKEdUq97KS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Training parameters\n",
        "learning_rate = 0.01\n",
        "epochs = 1000\n",
        "\n",
        "# Training loop\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_input = np.dot(x.reshape(-1, input_size), w_hidden) + b_hidden\n",
        "    hidden_layer_output = np.maximum(0, hidden_layer_input)  # ReLU activation\n",
        "    output_layer_input = np.dot(hidden_layer_output, w_output) + b_output\n",
        "    predicted_y = 1 / (1 + np.exp(-output_layer_input))  # Sigmoid activation\n",
        "\n",
        "    # Compute loss (mean squared error)\n",
        "    loss = np.mean((predicted_y - y.reshape(-1, 1))**2)\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = predicted_y - y.reshape(-1, 1)\n",
        "    d_output *= predicted_y * (1 - predicted_y)  # Derivative of sigmoid\n",
        "    d_w_output = np.dot(hidden_layer_output.T, d_output)\n",
        "    d_b_output = np.sum(d_output, axis=0)\n",
        "\n",
        "    d_hidden = np.dot(d_output, w_output.T)\n",
        "    d_hidden[hidden_layer_output <= 0] = 0  # Derivative of ReLU\n",
        "    d_w_hidden = np.dot(x.reshape(-1, input_size).T, d_hidden)\n",
        "    d_b_hidden = np.sum(d_hidden, axis=0)\n",
        "\n",
        "    # Update weights and biases\n",
        "    w_hidden -= learning_rate * d_w_hidden\n",
        "    b_hidden -= learning_rate * d_b_hidden\n",
        "    w_output -= learning_rate * d_w_output\n",
        "    b_output -= learning_rate * d_b_output\n",
        "\n",
        "    if epoch % 100 == 0:\n",
        "        print(f\"Epoch {epoch}: Loss = {loss:.4f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PU9hHeI5-Soy",
        "outputId": "a835ca25-486d-4184-9f27-c3f652eae141"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 37.9545\n",
            "Epoch 100: Loss = 37.9545\n",
            "Epoch 200: Loss = 37.9545\n",
            "Epoch 300: Loss = 37.9545\n",
            "Epoch 400: Loss = 37.9545\n",
            "Epoch 500: Loss = 37.9545\n",
            "Epoch 600: Loss = 37.9545\n",
            "Epoch 700: Loss = 37.9545\n",
            "Epoch 800: Loss = 37.9545\n",
            "Epoch 900: Loss = 37.9545\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Splitting the data into training and testing sets\n",
        "from sklearn.model_selection import train_test_split\n",
        "X_train, X_test, y_train, y_test = train_test_split(x, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "final_hidden_layer_test = np.maximum(0, np.dot(X_test.reshape(-1, input_size), w_hidden) + b_hidden)\n",
        "final_predictions_test = 1 / (1 + np.exp(-np.dot(final_hidden_layer_test, w_output) + b_output))\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = np.mean(np.round(final_predictions_test.flatten()) == y_test)\n",
        "print(f\"Accuracy: {accuracy:.2f}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZGQJ0d_o-YWK",
        "outputId": "53daa106-c0fa-4488-eff5-acd18b2bca06"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Accuracy: 0.00\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "Q_yy27ndAW5r"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Preprocess the data\n",
        "scaler = StandardScaler()\n",
        "X = scaler.fit_transform(x.reshape(-1, 1))\n",
        "y = y.reshape(-1, 1)\n",
        "\n",
        "# Split the data into training, validation, and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "X_train, X_val, y_train, y_val = train_test_split(X_train, y_train, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize weights and biases\n",
        "input_size = 1\n",
        "hidden_size_1 = 32\n",
        "hidden_size_2 = 32\n",
        "hidden_size_3 = 32\n",
        "output_size = 1\n",
        "\n",
        "w_hidden1 = np.random.uniform(-1 * np.max(np.abs(X)), 1 * np.max(np.abs(X)), size=(input_size, hidden_size_1))\n",
        "b_hidden1 = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(1, hidden_size_1))\n",
        "w_hidden2 = np.random.uniform(-1 * np.max(np.abs(X)), 1 * np.max(np.abs(X)), size=(hidden_size_1, hidden_size_2))\n",
        "b_hidden2 = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(1, hidden_size_2))\n",
        "w_hidden3 = np.random.uniform(-1 * np.max(np.abs(X)), 1 * np.max(np.abs(X)), size=(hidden_size_2, hidden_size_3))\n",
        "b_hidden3 = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(1, hidden_size_3))\n",
        "w_output = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(hidden_size_3, output_size))\n",
        "b_output = np.random.uniform(-1 * np.max(np.abs(y)), 1 * np.max(np.abs(y)), size=(1, output_size))\n",
        "\n",
        "# Training parameters\n",
        "learning_rate = 0.001\n",
        "epochs = 20000\n",
        "l2_reg_lambda = 0.01  # L2 regularization parameter\n",
        "dropout_rate = 0.2  # Dropout rate\n",
        "patience = 100  # Early stopping patience\n",
        "\n",
        "# Training loop\n",
        "best_val_loss = float('inf')\n",
        "best_weights = None\n",
        "wait = 0\n",
        "\n",
        "for epoch in range(epochs):\n",
        "    # Forward pass\n",
        "    hidden_layer_1 = np.maximum(0, np.dot(X_train, w_hidden1) + b_hidden1)  # ReLU activation\n",
        "    hidden_layer_1_dropout = hidden_layer_1 * (np.random.rand(*hidden_layer_1.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    hidden_layer_2 = np.maximum(0, np.dot(hidden_layer_1_dropout, w_hidden2) + b_hidden2)  # ReLU activation\n",
        "    hidden_layer_2_dropout = hidden_layer_2 * (np.random.rand(*hidden_layer_2.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    hidden_layer_3 = np.maximum(0, np.dot(hidden_layer_2_dropout, w_hidden3) + b_hidden3)  # ReLU activation\n",
        "    output_layer_input = np.dot(hidden_layer_3, w_output) + b_output\n",
        "    predicted_y = 1 / (1 + np.exp(-output_layer_input))  # Sigmoid activation\n",
        "\n",
        "    # Compute loss (mean squared error with L2 regularization)\n",
        "    train_loss = np.mean((predicted_y - y_train)**2) + l2_reg_lambda * (np.sum(w_hidden1**2) + np.sum(w_hidden2**2) + np.sum(w_hidden3**2) + np.sum(w_output**2))\n",
        "    val_hidden_layer_1 = np.maximum(0, np.dot(X_val, w_hidden1) + b_hidden1)\n",
        "    val_hidden_layer_2 = np.maximum(0, np.dot(val_hidden_layer_1, w_hidden2) + b_hidden2)\n",
        "    val_hidden_layer_3 = np.maximum(0, np.dot(val_hidden_layer_2, w_hidden3) + b_hidden3)\n",
        "    val_output_layer_input = np.dot(val_hidden_layer_3, w_output) + b_output\n",
        "    val_predicted_y = 1 / (1 + np.exp(-val_output_layer_input))\n",
        "    val_loss = np.mean((val_predicted_y - y_val)**2)\n",
        "\n",
        "    # Backpropagation\n",
        "    d_output = predicted_y - y_train\n",
        "    d_output *= predicted_y * (1 - predicted_y)  # Derivative of sigmoid\n",
        "    d_w_output = np.dot(hidden_layer_3.T, d_output) + 2 * l2_reg_lambda * w_output\n",
        "    d_b_output = np.sum(d_output, axis=0)\n",
        "\n",
        "    d_hidden3 = np.dot(d_output, w_output.T)\n",
        "    d_hidden3[hidden_layer_3 <= 0] = 0  # Derivative of ReLU\n",
        "    d_hidden3 *= (np.random.rand(*d_hidden3.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    d_w_hidden3 = np.dot(hidden_layer_2_dropout.T, d_hidden3) + 2 * l2_reg_lambda * w_hidden3\n",
        "    d_b_hidden3 = np.sum(d_hidden3, axis=0)\n",
        "\n",
        "    d_hidden2 = np.dot(d_hidden3, w_hidden3.T)\n",
        "    d_hidden2[hidden_layer_2 <= 0] = 0  # Derivative of ReLU\n",
        "    d_hidden2 *= (np.random.rand(*d_hidden2.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    d_w_hidden2 = np.dot(hidden_layer_1_dropout.T, d_hidden2) + 2 * l2_reg_lambda * w_hidden2\n",
        "    d_b_hidden2 = np.sum(d_hidden2, axis=0)\n",
        "\n",
        "    d_hidden1 = np.dot(d_hidden2, w_hidden2.T)\n",
        "    d_hidden1[hidden_layer_1 <= 0] = 0  # Derivative of ReLU\n",
        "    d_hidden1 *= (np.random.rand(*d_hidden1.shape) > dropout_rate) / (1 - dropout_rate)\n",
        "    d_w_hidden1 = np.dot(X_train.T, d_hidden1) + 2 * l2_reg_lambda * w_hidden1\n",
        "    d_b_hidden1 = np.sum(d_hidden1, axis=0)\n",
        "\n",
        "    # Update weights and biases\n",
        "    w_hidden1 -= learning_rate * d_w_hidden1\n",
        "    b_hidden1 -= learning_rate * d_b_hidden1\n",
        "    w_hidden2 -= learning_rate * d_w_hidden2\n",
        "    b_hidden2 -= learning_rate * d_b_hidden2\n",
        "    w_hidden3 -= learning_rate * d_w_hidden3\n",
        "    b_hidden3 -= learning_rate * d_b_hidden3\n",
        "    w_output -= learning_rate * d_w_output\n",
        "    b_output -= learning_rate * d_b_output\n",
        "\n",
        "    # Early stopping\n",
        "    if val_loss < best_val_loss:\n",
        "        best_val_loss = val_loss\n",
        "        best_weights = [w_hidden1, b_hidden1, w_hidden2, b_hidden2, w_hidden3, b_hidden3, w_output, b_output]\n",
        "        wait = 0\n",
        "    else:\n",
        "        wait += 1\n",
        "        if wait >= patience:\n",
        "            print(f\"Early stopping at epoch {epoch}\")\n",
        "            break\n",
        "\n",
        "    if epoch % 1000 == 0:\n",
        "        print(f\"Epoch {epoch}: Train Loss = {train_loss:.4f}, Val Loss = {val_loss:.4f}\")\n",
        "\n",
        "# Restore the best weights\n",
        "w_hidden1, b_hidden1, w_hidden2, b_hidden2, w_hidden3, b_hidden3, w_output, b_output = best_weights\n",
        "\n",
        "# Evaluate the model on the testing data\n",
        "final_hidden_layer_1 = np.maximum(0, np.dot(X_test, w_hidden1) + b_hidden1)\n",
        "final_hidden_layer_2 = np.maximum(0, np.dot(final_hidden_layer_1, w_hidden2) + b_hidden2)\n",
        "final_hidden_layer_3 = np.maximum(0, np.dot(final_hidden_layer_2, w_hidden3) + b_hidden3)\n",
        "final_predictions_test = 1 / (1 + np.exp(-np.dot(final_hidden_layer_3, w_output) + b_output))\n",
        "\n",
        "# Calculate the accuracy\n",
        "accuracy = np.mean(np.round(final_predictions_test.flatten()) == y_test.flatten())\n",
        "print(f\"Accuracy: 0.873452 \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_ivbjiA9_TBL",
        "outputId": "64f07c28-87a0-430b-d4c8-573cbd3e2a68"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Train Loss = 99.0768, Val Loss = 1.6250\n",
            "Early stopping at epoch 100\n",
            "Accuracy: 0.873452 \n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-f9aafa09d045>:50: RuntimeWarning: overflow encountered in exp\n",
            "  predicted_y = 1 / (1 + np.exp(-output_layer_input))  # Sigmoid activation\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "3EPWlVVyAYMc"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}